\documentclass[12pt, oneside, a4paper]{article}

\usepackage[left=3.5cm, top=2.5cm, bottom=2.5cm, right=2.5cm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{polski}
\usepackage[polish]{babel}
\usepackage[]{amsmath}
\usepackage[]{listings}
\usepackage[]{pgf}
\usepackage[]{float}
\usepackage[]{graphics}
\usepackage[]{svg}

% temporarily for dark background
\usepackage{xcolor}
\definecolor{mybackground}{rgb}{0.15, 0.15, 0.13}
\definecolor{mytext}{rgb}{0.94, 0.94, 0.92}
% \pagecolor{mybackground}
% \color{mytext}

\usepackage[
  color=orange!30,
  textcolor=black,
  figcolor=white,
  figwidth=10cm
]{todonotes}


\lstset{
  basicstyle=\ttfamily,
  captionpos=b,
  aboveskip=2em,
}

% listing settings
\renewcommand{\ttdefault}{pcr}
\lstdefinestyle{hls}{
  language=C++,
  keywordstyle=\ttfamily\bfseries,
  numbers=left,
  numberstyle=\small,
  xleftmargin=2em,
}

\def\CPP{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}


\begin{document}
\thispagestyle{empty}
\begin{titlepage}
    \begin{center}

      \Large
	    \textbf{Uniwersytet Jagielloński w Krakowie}\vspace{0.2cm}\\ Wydział Fizyki, Astronomii i Informatyki Stosowanej
      \vspace*{1cm}
               
      \vspace{3cm}
      \Large
      \textbf{Wojciech Lepich}\\\vspace{0.5cm}
      \normalsize Nr albumu: 1146600\\
      \vspace{2cm}
      \Huge
      \textbf{Rozpoznawanie cyfr przez sieć neuronową zaimplementowaną na układzie FPGA}
      
      \vspace{1.5cm}
      \normalsize
      Praca licencjacka\\
      na kierunku Informatyka\\ \vspace{0.15cm}
        
      \vfill
      \vspace{2cm}
      \begin{minipage}{1\textwidth}
\begin{flushright}
Praca wykonana pod kierunkiem\\
dr. Grzegorza Korcyla\\
z Zakładu Technologii Informatycznych
\end{flushright}
\end{minipage}
        
        \vspace{2cm}
        \begin{center}
      Kraków 2020
        \end{center}
    \end{center}
\end{titlepage}

\newpage 
\thispagestyle{empty}
\vspace{2.5cm}
\begin{flushleft}
\large \textbf{Oświadczenie autora pracy}\vspace{0.6cm}\\
\end{flushleft}

\noindent Świadom odpowiedzialności prawnej oświadczam, że niniejsza praca dyplomowa została napisana przeze mnie samodzielnie i nie zawiera treści uzyskanych w sposób niezgodny z obowiązującymi przepisami.\\

\noindent Oświadczam również, że przedstawiona praca nie była wcześniej przedmiotem procedur związanych z uzyskaniem tytułu zawodowego w wyższej uczelni.
\vspace{2cm}
\begin{center}
\begin{tabular}{lr}
\ldots\ldots\ldots\ldots\ldots\ldots~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&
\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots \\
{~~~~Kraków, dnia} & {Podpis autora pracy~~~~}
\end{tabular}
\end{center}
\vspace{5cm}
\begin{flushleft}
\large \textbf{Oświadczenie kierującego pracą}
\end{flushleft}

\noindent Potwierdzam, że niniejsza praca została przygotowana pod moim kierunkiem i~kwalifikuje się do przedstawienia jej w postępowaniu o nadanie tytułu zawodowego.
\vspace{2cm}
\begin{center}
\begin{tabular}{lr}
\ldots\ldots\ldots\ldots\ldots\ldots~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&
\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots \\
{~~~~Kraków, dnia} & {Podpis kierującego pracą~~}
\end{tabular}
\end{center}
\vfill

\newpage
\tableofcontents

\newpage
\section{Wstęp}
Tutaj wstęp

\newpage
\section{Teoria}
\subsection{Architektura FPGA}
Field-Programmable Gate Array (FPGA) to układy scalone, które mogą być
elektronicznie przeprogramowane bez potrzeby demontażu samego układu
z urządzenia. W porównaniu do układów ASIC znacznie taniej zaprojektować
pierwszy działający układ. Elastyczna natura układów FPGA wiąże się z większym
zużyciem powierzchni krzemu, opóźnień oraz zużycia energii.
\todo{Dodać przypis} (FPGA architecture: survey and challenges)

Podstawowa struktura układów FPGA składa się z różnych bloków logicznych,
które mogą być łączone ze sobą w zależności od wymagań projektowych.
Przykładami takich bloków są:
DSP (jednostka przeprowadzająca obliczenia dodawania/mnożenia),
LUT (Look-Up Table, de facto tablica prawdy dowolnej funkcji boolowskiej),
Flip Flop (przechowują wynik LUT), BRAM (Block RAM, pamięć dwuportowa,
jest w stanie przechowywać względnie dużą ilość danych).

Układy FPGA przeważnie pracują na kilku-, kilkunastukrotnie niższych
częstotliwościach niż CPU. Osiągają wysoką wydajność
dzięki masywnemu zrównolegleniu obliczeń.

Programowanie FPGA polega na pisaniu logiki
w językach HDL (Hardware Description Language) takimi jak VHDL 
czy też Verilog. Napisana logika definiuje zachowanie układu FPGA.
Gotowy opis logiki syntetyzuje się, czyli generuje połączenia pomiędzy
zasobami układu. Kolejnym etapem jest implementacja --- odzworowanie
połączeń w konkretnym układzie.
 
HLS (High-Level Synthesis) to proces ułatwiający pisanie skomplikowanej
logiki. Algorytmy można pisać w językach wysokiego poziomu, takich jak
C, \CPP, SystemC. Przygotowany kod jest transpilowany poprzez odpowiedni
kompilator HLS do języka RTL (Register-Transfer Level; język opisu sprzętu
na poziomie bramek i rejestrów), a ten może być zaimplementowany na układzie.

\subsection{Przetwarzanie obrazu}
Cyfrowe przetwarzanie obrazu jest problemem wymagającym dużych mocy
obliczeniowych ze względu na ilość danych do przetworzenia. Nieskompresowany
kolorowy obraz z pikselami w formacie RGB (po 8 bitów na kolor) o wysokości
720 pikseli i szerokości 1280 pikseli to 22118400 bitów (\(\approx \) 2,5MB). Obraz
przetwarzany w czasie rzeczywistym, na przykład z kamery, zwielokrotnia tę
liczbę o liczbę klatek na sekundę (przy trzydziestu klatkach na sekundę liczba
danych rośnie do około 79 megabajtów na sekundę). Należy również pamiętać, że
dane są dwuwymiarowe co jest ważne przy problemach związanych z rozpoznawaniem
wzorców, klasyfikacją przedmiotów na obrazie, filtrowania w celu rozmazania lub
wyostrzenia obrazów, itp.

\subsubsection{Formaty pikseli}
Jest wiele modeli przestrzeni barw (a co za tym idzie, sposobów kodowania
pikseli) między innymi:
\begin{itemize}
  \item RGB, używany w aparatach, skanerach, telewizorach
  \item CMYK, używany w druku wielobarwnym
  \item HSV
  \item YUV
\end{itemize}
Składowe dwóch ostatnich przestrzeni barw oddzielają informację o jasności
od informacji o kolorach. Model barw YUV składa się z kanału luminacji Y
oraz kanałów kodujących barwę U oraz V, są to kolejno składowa niebieska
i składowa czerwona. W projekcie użyty jest format pikseli YUY2 (znany też
pod nazwą YUYV), w którym na dwa piksele przypadają 32 bity.
\begin{figure}[h]
  \centering
  \includegraphics[scale=1.2]{figures/yuv2-scheme.png} 
  \caption{Schemat formatu pikseli YUV2}\label{fig:yuv2}
\end{figure}
Licząc od najstarszego bitu pierwsze osiem bitów przypada na Y0, to jest
luminacja pierwszego piksela, następne osiem bitów na U0, kolejne osiem bitów
to luminacja drugiego piksela, a pozostałe bity to składowa czerwona V0.
Dla obydwóch pikseli składowe U i V są wspólne. Co istotne w projekcie,
łatwo oddzielić luminację, która jest używana w przetwarzaniu obrazu.

\subsection{Sieci neuronowe}
Sztuczna sieć neuronowa (SSN) jest modelem zdolnym do odwzorowania złożonych
funkcji. Najprostsze sieci są zbudowane ze sztucznych neuronów, z których każdy
posiada wiele wejść oraz jedno wyjście, które może być połączone z wejściami
wielu innych neuronów. Każde z wejść neuronu jest związane ze znalezioną
w procesie trenowania wagą. Wartość wyjścia to obliczony wynik funkcji aktywacji
z sumy ważonych wejść. Sieć może mieć wiele warstw neuronów ukrytych, których
wejściami są wyjścia neuronów z poprzedniej warstwy. 

Sieci neuronowe są stosowane w problemach
związanych z predykcją, klasyfikacją, przetwarzaniem i analizowaniem
danych. Do ich zastosowania nie jest potrzebna znajomość algorytmu rozwiązania
danego problemu. Obliczenia w sieciach są wykonywane równolegle w każdej
warstwie, dzięki czemu implementacja sieci na układzie FPGA może działać
wielokrotnie szybciej niż na CPU, pomimo niższej częstotliwości układu.

\newpage
\section{Opis projektu}

\subsection{Zarys projektu}
Celem projektu jest implementacja systemu do rozpoznawania cyfr
w czasie rzeczywistym. Cel zrealizowano poprzez implementację
wtyczki GStreamera, wykorzystującej sieć neuronową, na układzie
Xilinx Zynq MPSoC oraz stworzenie odpowiedniego potoku danych
korzystając z bibliotek GStreamer. Zadaniami spoczywającymi na innych
elementach potoku jest obsługa kamery,
kadrowanie i skalowanie obrazu oraz wyświetlenie go na końcowym urządzeniu.

\subsection{Platforma}
\missingfigure{Zdjęcie stanowiska}
Sprzęt wykorzystany w projekcie to Xilinx Zynq UltraScale+ MPSoC ZCU104.
Na jednym układzie znajduje się czterordzeniowy procesor
ARM \mbox{Cortex-A53},
dwurdzeniowy procesor ARM \mbox{Cortex-R5},
układ graficzny \mbox{Mali-400} oraz zasoby FPGA.
Całość projektu została oparta o platformę Xilinx reVISION. Przetwarzane
dane dostarczane są z kamery USB, która była dołączona w zestawie z płytą Zynq.
Urządzeniem końcowym jest telewizor połączony przewodem HDMI z płytą.

\subsection{Sieć neuronowa}
Architektura sieci została dobrana uwzględniając dostępne zasoby programowalnej
logiki na płycie, a także możliwości sprzętu na którym dokonywana była jej
synteza. Dla problemu klasyfikowania obrazów dobrze nadają się sieci
splotowe (konwolucyjne, ang.\ convolutional neural networks --- CNN), 
\todo{przypis} których przykładem jest popularna sieć \mbox{LeNet-5}.
Architektura ta zawiera zarówno w pełni połączone warstwy
oraz warstwy splotowe i łączące.
Niestety z powodu ograniczeń sprzętowych w pracy nie została użyta ta
architektura.

\begin{figure}[h]
  \centering
  % \includegraphics{figures/network-scheme.png}
  \includesvg[scale=0.625]{figures/nn-scheme.svg}
  \caption{Schemat użytej architektury.
  }\label{fig:nn-scheme}
\end{figure}
Model wykorzystany w projekcie posiada 2 warstwy ukryte, posiadające kolejno
12 i 40 neuronów aktywowanych funkcją ReLU, i warstwy wyjściowej złożonej
z 10 neuronów z funkcją aktywacji softmax. Do stworzenia sieci wykorzystano
bibliotekę TensorFlow. Sieć uczona była na danych
z \todo{przypis}bazy MNIST składającej się łącznie z 70000 przykładów
cyfr na obrazach o wielkości 28\(\times \)28 pikseli, z których każdy
przedstawiony jest jako wartość od 0 (kolor czarny) do 255 (kolor biały).
Próbki zostały
podzielone na zbiór uczący, liczący 60000 próbek, oraz zbiór do testów
z pozostałych 10000 cyfr. Cyfry składają się z białych pikseli, tło jest czarne.
Dokonano prób trenowania sieci przetworzonymi danymi, w których kolory były
odwrócone, natomiast wytrenowane modele w trakcie testów nie przekraczały
progu czterdziestu procent dobrze zaklasyfikowanych obrazów.

\subsection{hls4ml}
\subsubsection{Idea hls4ml}
\begin{figure}[h]
  \centering
  \todo[inline]{Wstawić źródło w przypisie}
  \includegraphics[scale=0.2]{figures/hls4ml.jpg}
  \caption{Schemat pracy z hls4ml}\label{fig:hls4ml}
\end{figure}
Celem projektu hls4ml jest wygenerowanie kodu \CPP{} na podstawie zapisanego
modelu z TensorFlow.
Czerwona część schematu pokazuje ogólną organizację pracy przy projektowaniu 
odpowiedniego modelu uczenia maszynowego. Niebieska część należy do hls4ml,
który tłumaczy dostarczony model z wagami do syntetyzowalnego kodu, który
następnie można włączyć do większego projektu lub zaimplementować jako
samodzielną część na FPGA. Generowany projekt jest parametryzowany
przez plik konfiguracyjny yml zawierający ścieżkę do pliku z zapisanym modelem
wraz z wagami, typy danych kodujące wartości wag, nazwę docelowego układu FPGA
raz parametry optymalizacji dotyczące zużycia zasobów --- większa ilość
zasobów oznacza zrównoleglenie większej części obliczeń.

\subsubsection{Precyzja danych}
Typ danych używany w przekonwertowanym modelu to duże liczby
stałoprzecinkowe (\lstinline{ap_fixed}) oraz liczby całkowite
(\lstinline{ap_int}).
Precyzję obu typów można ustalić do jednego bita.
Obliczenia przeprowadzane na liczbach o mniejszej
precyzji są umożliwiają większe zrównoleglenie obliczeń,
natomiast zbyt niska precyzja może poskutkować
bezużytecznością zsyntetyzowanej sieci. Aby odpowiednio dobrać precyzję
wag skorzystano z pythonowej biblioteki hls4ml.profiling.

Program korzystający z funkcji dostarczanych przez tę bibliotekę
analizuje plik konfiguracyjny yml oraz model z pliku h5.
Wynikiem działania programu jest wykres przedstawiający
rozkład wartości wag każdej z warstw modelu otrzymanych w procesie trenowania.
Szare pole w tle wykresu
przedstawia zakres wartości, które obejmowane są przez precyzję określoną
w pliku konfiguracyjnym. Dobrym punktem początkowym jest wybranie takiej
liczby bitów dla każdej z warstw, która obejmuje wszystkie możliwe wagi.
Dalsze ustalanie precyzji można wykonać w trakcie analizy wyników
symulacji.
\begin{figure}[H]
  \input{figures/dist_of_weights.pgf}
  \caption{Rozkład wartości wag modelu}\label{fig:weights_dist}
  \centering
\end{figure}


\subsection{GStreamer}
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.24]{figures/image_flow.png}
  \caption{Schemat logiczny przetwarzania obrazu}\label{fig:image_flow}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5625]{figures/pipeline.png}
  \caption{Schemat grafu. Na zielono plugin z siecią neuronową}\label{fig:pipeline}
\end{figure}
Zsyntetyzowana sieć jest częścią projektu. Potrzebne jest również dostarczenie
danych do sieci oraz przedstawienie wyniku.
Do tego celu skorzystano z biblioteki GStreamer, dzięki której można
tworzyć grafy z komponentów (pluginów, elementów)
przetwarzających media, zarówno audio jak i video.
Każdy z elementów grafu składa się z co najmniej jednego źródła (source),
lub ujścia (sink), może mieć również wiele wejść i wyjść. W grafie pierwszy
element nie może mieć wejść, natomiast konieczne jest aby posiadał co najmniej
jedno wyjście. Poprawnie przygotowany graf nie powinien mieć komponentów
oferujących źródło, które nie są z niczym połączone.
Pluginy mają ujednolicony interfejs, dzięki czemu można w łatwy sposób
włączyć do grafu własny element. Wtyczki charakteryzują się
pewnymi własnościami, znanymi jako „caps”. Określają one jakie 
media jest w stanie przetworzyć dana wtyczka (na przykład format pikseli,
maksymalny rozmiar obrazu). 
Łączone ze sobą elementy dokonują negocjacji
parametrów mediów, takich jak rozdzielczość obrazu, format pikseli,
ilość klatek na sekundę oraz innych.

\subsubsection{xlnxvideosrc i xlnxvideosink}
Są to pluginy dostarczone przez firmę Xilinx wraz z platformą reVISION.
Obydwa korzystają biblioteki Xilinx \lstinline{video_lib}
Pierwszy z nich ułatwia odczytywanie danych ze źródeł, dla których potrzebne
byłyby dodatkowe działania. Są to między innymi kamera USB (użyta w projekcie),
HDMI, MIPI CSI (sprzętowy interfejs do transmisji obrazów i wideo).
\todo{Przypis UG1221, s.32}
Sam element zbudowany jest w oparciu o element v4l2src, dostępny
w standardowej instalacji GStreamera.
Xlnxvideosink również jest oparty o inny element --- kmssink.
Zapewnia odpowiednią konfigurację połączenia z wyświetlaczami
podłączonymi przez HDMI oraz DisplayPort.

\subsubsection{videoconvert}
Element mający za zadanie dostosować wszystkie parametry obrazu tak,
aby móc połączyć ze sobą dwa niekompatybilne pod względem „caps” elementy.
Ta niekompatybilność może być spowodowana na przykład tym, że dwie
wtyczki potrzebują innego formatu pikseli i jednocześnie nie oferują
możliwości konwersji z jednego formatu na inny.

\subsubsection{videocrop}
Wtyczka służąca do wykadrowania obrazu w zdefiniowanym obszarze.
Wykorzystana została aby otrzymać obraz o tej samej długości
i szerokości wynoszącej 224 (co jest ośmiokrotnością 28, czyli
długością boku obrazów, którymi wytrenowana została sieć) wycięty
ze środka wideo o rozmiarze 1920\(\times \)1080.

\subsubsection{videoscale}
Skaluje obraz do wynegocjowanych pomiędzy sąsiadującymi elementami
parametrów, przy czym pierwsza próba negocjacji to ta sama wielkość
obrazu przy ujściu jak i w źródle, aby skalowanie nie było potrzebne.

\subsubsection{sdxnet}
Sdxnet to wtyczka wykorzystująca sieć neuronową do rozpoznania cyfr
znajdujących się na obrazie przez nią przechodzącym. Element ten został
zaimplementowany na potrzeby tego projektu.

\subsubsection{Filter caps}
Element precyzujący parametry obrazu, które wymuszają
dostosowanie się poprzedniego elementu --- na przykład videoscale.
Zapisuje się go w postaci ciągu znaków ujętych w cudzysłów.

\subsubsection{videobox}
Oferuje możliwość osadzenia obrazu w tym o innym rozmiarze
rozmiarze wypełniając pozostałą przestrzeń ramką
w wybranym kolorze. Własność autocrop oznacza automatyczne obliczenie
wielkości ramek na podstawie parametrów określonych przez kolejny element tak,
aby obraz przychodzący do videobox był wycentrowany a ramki
były tej samej wielkości.

\subsubsection{fpsdisplaysink}
Wtyczka typu sink (mająca tylko ujście), która jako parametr
pobiera inną wtyczkę tego typu, np.~xlnxvideosink,
zastępując w grafie tamtą.
Jej użycie pozwala na sprawdzenie liczby klatek na sekundę
wyświetlanego obrazu.


\todo[inline]{Todo: schemat fizyczny: hdmi -> arm (wtyczki) ->
dane do sieci -> ...}

\subsection{Używanie sieci}
\subsubsection{Generowanie projektu}
Architekturę sieci wraz z wagami zapisano do pliku h5. Stworzono
plik konfiguracyjny hls4ml. Następnie na jego podstawie wygenerowano projekt
z przekonwertowaną siecią. Wśród wygenerowanych plików znajduje się
również kod służący do symulacji działania projektu. Przygotowane zostały
pliki z danymi testującymi sieć --- 10000 przetworzonych przykładów z bazy
MNIST tak, aby cyfry były koloru czarnego, tło białego. Zakres wartości
wynosi od 0 do 255.

\subsubsection{Funkcja}
Cała sieć jest przedstawiona jako jedna funkcja.
Parametrami tej funkcji są dwie tablice:
\lstinline[style=hls]{input[]}, do której są zapisywane są
dane do przetworzenia,
oraz \lstinline[style=hls]{output[]}, do której funkcja zapisuje
obliczone predykcje.

\begin{minipage}{\linewidth}
\lstinputlisting[
  caption={Nagłówek funkcji},
  label={lst:nn_header},
  style=hls
]{listings/nn.h}
\end{minipage}

\subsubsection{Interfejs}
Aby móc korzystać z sieci w aplikacji uruchamianej na procesorze ARM
zadeklarowano użycie interfejsu IO \mbox{AXI-4} Lite.

\subsubsection{Dostosowanie sieci}
W celu poprawienia wyników działania sieci dokonano pewnych usprawnień.
Sieć została wytrenowana oryginalnymi danymi, w których piksele tworzące
cyfry mają wartości równe 255 lub tej wartości bliskie,
a piksele białego są przedtawione jako 0.
Ponadto rzeczywiste dane z kamery mogą być zaszumione,
przedstawione obiekty zacienione, a same cyfry mogą nie być idealnie czarne.

Przy każdym wywołaniu funkcji sieci dokonywana jest transformacja
danych poprzez kod pokazany na listingu~\ref{lst:data_tresh}.
Wartość każdego piksela jest zamieniana na wartość 255 lub 0, zależnie
od początkowej jego wartości --- dla wartości mniejszych od 140
(kolor szary lub ciemniejszy) przypisany jest kolor biały, wartość 255.
Dla pikseli jasnych (od 140 w górę) przypisywana wartość to 0, kolor czarny.

W ten sposób dokonuje się zarówno odpowiedniego przetworzenia danych
uwzględniającego sposób wytrenowania modelu, jak również uwydatnienia
cyfry oraz pozbycia się szumów obrazu i jasnych cieni.

\hspace{-1cm}
\begin{minipage}{\linewidth}
\lstinputlisting[
  caption={Transformacja danych.},
  label={lst:data_tresh},
  firstline=36,
  lastline=39,
  language=C++
  % floatplacement=H
]{listings/nn.cpp}
\begin{itemize}
  \setlength{\itemindent}{3em}
  \item \lstinline[style=hls]{input[]} --- tablica z danymi (parametr funkcji)
  \item \lstinline[style=hls]{input1[]} --- dane przetwarzane przez sieć
\end{itemize}
\end{minipage}

\subsubsection{Tworzenie biblioteki}
\begin{figure}[h]
  \centering
  \includesvg[scale=0.5]{figures/sdx_pack-scheme}
  \caption{Schemat tworzenia biblioteki}\label{fig:ccall}
\end{figure}
Zsyntetyzowany moduł sieci został wyeksportowany w środowisku Vivado HLS
do paczki IP (Intelectual Property). Następnie poprzez narzędzie
\lstinline{sdx_pack} utworzono statyczną bibliotekę gotową do wykorzystania
w innym projekcie w środowisku SDSoC (Software-Defined System on Chip,
IDE do pisania aplikacji lub bibliotek uruchamianych na platformach
Xilinx MPSoC).

\begin{minipage}{\linewidth}
\lstinputlisting[
  caption={Narzędzie sdx\_pack},
  label={lst:sdx_pack},
]{listings/sdx_pack.sh}
\end{minipage}

Wywołując narzędzie \lstinline{sdx_pack} należy podać plik nagłówkowy funkcji,
docelową nazwę biblioteki, mapowanie parametrów funkcji na porty modułu,
ścieżkę do pliku \lstinline{component.xml} wygenerowanego podczas eksportu
do paczki IP, protokół kontroli modułu, odpowiedni zegar, a informacje
dotyczące docelowej platformy: nazwę jej rodziny, procesora oraz systemu.

\subsection{Część neuralnet}
Czytanie obrazu, podział na część luma i chroma, wywołanie funkcji sieci,
zapis z powrotem, synteza do biblioteki dzielonej „.so”

\subsection{Część gstsdxnet}
De facto plugin gstreamera, w którym są wywoływane funkcje z biblioteki
dzielonej neuralnet.so, 

\subsection{Małe podsumowanie}


\newpage
\section{Wyniki i dyskusja}

\subsection{Ewaulacja modelu}
Wyniki z samego pythona z danymi testowymi z mnista

\subsection{Symulacja}
Tutaj wyniki z symulacji z danymi testowymi z mnista

\subsection{Dane rzeczywiste}
Wyniki z kamerki. Zdjęcia danych testowych, co wpływa na wynik, czy wszystko
rozpoznaje itd,

\newpage
\section{Podsumowanie}
W projekcie zostało zrobione to i to. Wyszło to tak i tak. Problem sprawiło
tamto i owamto. Można to poprawić w ten sposób. Można część funkcjonalności
z pipeline przenieść na fpga (w końcu przetwarzanie obrazu na fpga jest szybkie)

\end{document}