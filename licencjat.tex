\documentclass[12pt, oneside]{article}

\usepackage[left=3.5cm, top=2.5cm, bottom=2.5cm, right=2.5cm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{polski}
\usepackage[polish]{babel}
\usepackage[]{amsmath}
\usepackage[]{listings}

\usepackage[color=gray!30, figcolor=white]{todonotes}

% temporarily for dark background
\usepackage{xcolor}
\definecolor{mybackground}{rgb}{0.15, 0.15, 0.13}
\definecolor{mytext}{rgb}{0.94, 0.94, 0.92}
\pagecolor{mybackground}
\color{mytext}

\begin{document}  
\thispagestyle{empty}
\begin{titlepage}
    \begin{center}

      \Large
	    \textbf{Uniwersytet Jagielloński w Krakowie}\vspace{0.2cm}\\ Wydział Fizyki, Astronomii i Informatyki Stosowanej
      \vspace*{1cm}
               
      \vspace{3cm}
      \Large
      \textbf{Wojciech Lepich}\\\vspace{0.5cm}
      \normalsize Nr albumu: 1146600\\
      \vspace{2cm}
      \Huge
      \textbf{Rozpoznawanie cyfr przez sieć neuronową zaimplementowaną na układzie FPGA}
      
      \vspace{1.5cm}
      \normalsize
      Praca licencjacka\\
      na kierunku Informatyka\\ \vspace{0.15cm}
        
      \vfill
      \vspace{2cm}
      \begin{minipage}{1\textwidth}
\begin{flushright}
Praca wykonana pod kierunkiem\\
dr. Grzegorza Korcyla\\
z Zakładu Technologii Informatycznych
\end{flushright}
\end{minipage}
        
        \vspace{2cm}
        \begin{center}
      Kraków 2020
        \end{center}
    \end{center}
\end{titlepage}

\newpage 
\thispagestyle{empty}
\vspace{2.5cm}
\begin{flushleft}
\large \textbf{Oświadczenie autora pracy}\vspace{0.6cm}\\
\end{flushleft}

\noindent Świadom odpowiedzialności prawnej oświadczam, że niniejsza praca dyplomowa została napisana przeze mnie samodzielnie i nie zawiera treści uzyskanych w sposób niezgodny z obowiązującymi przepisami.\\

\noindent Oświadczam również, że przedstawiona praca nie była wcześniej przedmiotem procedur związanych z uzyskaniem tytułu zawodowego w wyższej uczelni.
\vspace{2cm}
\begin{center}
\begin{tabular}{lr}
\ldots\ldots\ldots\ldots\ldots\ldots~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&
\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots \\
{~~~~Kraków, dnia} & {Podpis autora pracy~~~~}
\end{tabular}
\end{center}
\vspace{5cm}
\begin{flushleft}
\large \textbf{Oświadczenie kierującego pracą}
\end{flushleft}

\noindent Potwierdzam, że niniejsza praca została przygotowana pod moim kierunkiem i~kwalifikuje się do przedstawienia jej w postępowaniu o nadanie tytułu zawodowego.
\vspace{2cm}
\begin{center}
\begin{tabular}{lr}
\ldots\ldots\ldots\ldots\ldots\ldots~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&
\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots \\
{~~~~Kraków, dnia} & {Podpis kierującego pracą~~}
\end{tabular}
\end{center}
\vfill

\newpage
\tableofcontents

\newpage
\section{Wstęp}
Tutaj wstęp

\newpage
\section{Teoria}
\subsection{Architektura FPGA}
Field-programmable gate array (FPGA) to układy scalone, które mogą być
elektronicznie przeprogramowane bez potrzeby demontażu samego układu
z urządzenia. W porównaniu do układów ASIC znacznie taniej zaprojektować
pierwszy działający układ. Elastyczna natura układów FPGA wiąże się z większym
zużyciem powierzchni krzemu, opóźnień oraz zużycia energii.
\todo{Dodać przypis} (FPGA architecture: survey and challenges)

Podstawowa struktura układów FPGA składa się z różnych bloków logicznych,
które mogą być łączone zależnie od projektu. Przykładami takich bloków są:
DSP (jednostka przeprowadzająca obliczenia dodawania/mnożenia),
LUT (look-up table, de facto tablica prawdy dowolnej funkcji boolowskiej),
Flip Flop (przechowują wynik LUT), BRAM (block RAM, pamięć dwuportowa,
jest w stanie przechowywać względnie dużą ilość danych).

Układy FPGA przeważnie pracują na kilku-, kilkunastukrotnie niższych
częstotliwościach niż CPU. \todo{Przypis} Wysoką wydajność zawdzięczają
zrównolegleniu obliczeń.

\subsection{Przetwarzanie obrazu}
Cyfrowe przetwarzanie obrazu jest problemem wymagającym dużych mocy
obliczeniowych ze względu na ilość danych do przetworzenia. Nieskompresowany
kolorowy obraz z pikselami w formacie RGB (po 8 bitów na kolor) o wysokości
720 pikseli i szerokości 1280 pikseli to 22118400 bitów (\(\approx \) 2,5MB). Obraz
przetwarzany w czasie rzeczywistym, na przykład z kamery, zwielokrotnia tę
liczbę o liczbę klatek na sekundę (przy trzydziestu klatkach na sekundę liczba
danych rośnie do około 79 megabajtów na sekundę). Należy również pamiętać, że
dane są dwuwymiarowe co jest ważne przy problemach związanych z rozpoznawaniem
wzorców, klasyfikacją przedmiotów na obrazie, filtrowania w celu rozmazania lub
wyostrzenia obrazów, itp.

\subsubsection{Formaty pikseli}
Jest wiele modeli przestrzeni barw (a co za tym idzie, sposobów kodowania
pikseli) między innymi:
\begin{itemize}
  \item RGB, używany w aparatach, skanerach, telewizorach
  \item CMYK, używany w druku wielobarwnym
  \item HSV
  \item YUV
\end{itemize}
Składowe dwóch ostatnich przestrzeni barw oddzielają informację o jasności
od informacji o kolorach. Model barw YUV składa się z kanału luminacji Y
oraz kanałów kodujących barwę U oraz V, są to kolejno składowa niebieska
i składowa czerwona. W projekcie użyty jest format pikseli YUY2 (znany też
pod nazwą YUYV), w którym na dwa piksele przypadają 32 bity.
\missingfigure[]{Schemat formatu pikseli YUV}
Licząc od najstarszego bitu pierwsze osiem bitów przypada na Y0, to jest
luminacja pierwszego piksela, następne osiem bitów na U0, kolejne osiem bitów
to luminacja drugiego piksela, a pozostałe bity to składowa czerwona V0.
Dla obydwóch pikseli składowe U i V są wspólne. Co istotne w projekcie,
łatwo oddzielić luminację, która jest używana w przetwarzaniu obrazu.

\subsection{Sieci neuronowe}
\todo{Schemat sieci}
Sztuczna sieć neuronowa (SSN) jest modelem zdolnym do odwzorowania złożonych
funkcji. Najprostsze sieci są zbudowane ze sztucznych neuronów, z których każdy
posiada wiele wejść oraz jedno wyjście, które może być połączone z wejściami
wielu innych neuronów. Każde z wejść neuronu jest związane ze znalezioną
w procesie trenowania wagą. Wartość wyjścia to obliczony wynik funkcji aktywacji
z sumy ważonych wejść. Sieć może mieć wiele warstw neuronów ukrytych, których
wejściami są wyjścia neuronów z poprzedniej warstwy. 

Sieci neuronowe są stosowane \todo{Przypis} w problemach
związanych z predykcją, klasyfikacją, przetwarzaniem i analizowaniem
danych. Do ich zastosowania nie jest potrzebna znajomość algorytmu rozwiązania
danego problemu. Obliczenia w sieciach są wykonywane równolegle w każdej
warstwie, dzięki czemu implementacja sieci na układzie FPGA może działać
wielokrotnie szybciej niż na CPU, pomimo niższej częstotliwości układu.

\newpage
\section{Opis projektu}

\subsection{Zarys projektu}
\todo{Schemat nr 1}
Celem projektu jest napisanie wtyczki do frameworka GStreamer
wykorzystującej sieć neuronową do rozpoznawania cyfr w czasie rzeczywistym 
na układzie Xilinx Zynq MPSoC oraz stworzenie odpowiedniego potoku danych
korzystając z bibliotek GStreamer. Zadaniami spoczywającymi na innych
elementach potoku jest obsługa kamery,
kadrowanie i skalowanie obrazu oraz wyświetlenie go na końcowym urządzeniu.

\subsection{Platforma}
\todo{Zdjęcia stanowiska}
Sprzęt wykorzystany w projekcie to Xilinx Zynq UltraScale+ MPSoC ZCU104.
Na jednym układzie znajduje się czterordzeniowy procesor
ARM \mbox{Cortex-A53},
dwurdzeniowy procesor ARM \mbox{Cortex-R5},
układ graficzny \mbox{Mali-400} oraz zasoby FPGA.
Całość projektu została oparta o platformę Xilinx reVISION. Przetwarzane
dane dostarczane są z kamery USB, która była dołączona w zestawie z płytą Zynq.
Urządzeniem końcowym jest telewizor połączony przewodem HDMI z płytą.

\subsection{Sieć neuronowa}
Architektura sieci została dobrana uwzględniając dostępne zasoby programowalnej
logiki na płycie, a także możliwości sprzętu na którym dokonywana była jej
synteza. Dla problemu klasyfikowania obrazów dobrze nadają się sieci
splotowe (konwolucyjne, ang.\ convolutional neural networks --- CNN), 
\todo{przypis} których przykładem jest popularna sieć \mbox{LeNet-5}.
Architektura ta zawiera zarówno w pełni połączone warstwy
oraz warstwy splotowe i łączące.
Niestety z powodu ograniczeń sprzętowych w pracy nie została użyta ta
architektura.

Model wykorzystany w projekcie posiada 2 warstwy ukryte, posiadające kolejno
12 i 40 neuronów aktywowanych funkcją ReLU, i warstwy wyjściowej złożonej
z 10 neuronów z funkcją aktywacji softmax. Do stworzenia sieci wykorzystano
bibliotekę TensorFlow. Sieć uczona była na danych
z \todo{przypis}bazy MNIST składającej się łącznie z 70000 przykładów
cyfr na obrazach o wielkości 28\(\times \)28 pikseli, z których każdy
przedstawiony jest jako wartość od 0 (kolor czarny) do 255 (kolor biały).
Próbki zostały
podzielone na zbiór uczący, liczący 60000 próbek, oraz zbiór do testów
z pozostałych 10000 cyfr.

Oryginalnie cyfry są białe na czarnym tle co zwiększa
\todo{przypis - badania własne} dokładność działania sieci (więcej 0 w danych),
natomiast trzeba wziąć to pod uwagę przy późniejszym wykorzystaniu sieci,
ponieważ docelowo sieć ma rozpoznawać czarne cyfry na białym tle.

\subsection{hls4ml}
\subsubsection{Idea hls4ml}
Celem projektu hls4ml jest automatyczne przetłumaczenie wytrenowanego modelu,
architektury i wag, do projektu syntezy wysokiego poziomu (HLS).
\todo{Tutaj schemat hls4ml workflow}
Czerwona część schematu pokazuje ogólną organizację pracy przy projektowaniu 
odpowiedniego modelu uczenia maszynowego. Niebieska część należy do hls4ml,
który tłumaczy dostarczony model z wagami do syntetyzowalnego kodu, który
następnie można włączyć do większego projektu lub zaimplementować jako
samodzielną część na FPGA.

\subsubsection{Precyzja danych}
Typ danych używany w przekonwertowanym modelu to duże liczby całkowite
(\lstinline{ap_int}) oraz liczby stałoprzecinkowe (\lstinline{ap_fixed}).
Precyzję obu można ustalić do jednego bita.
Obliczenia przeprowadzane na liczbach o mniejszej
precyzji są szybsze, natomiast zbyt niska może poskutkować
bezużytecznością zsyntetyzowanej sieci. Aby odpowiednio dobrać precyzję
danych skorzystano z narzędzi analizujących dostarczonych przez hls4ml.
Narzędzie jest uruchamiane w Pythonie, gdzie należy dostarczyć plik z modelem
Tensorflow, plik konfiguracyjny hsl4ml oraz dane.

Wynikiem są wykresy przedstawiające \todo{Wstaw wykres z profiling}
rozkład wartości wag każdej z warstw modelu. Szare pole w tle wykresu
przedstawia zakres wartości, które obejmowane są przez precyzję określoną
w pliku konfiguracyjnym. Dobrym punktem początkowym jest wybranie takiej
liczby bitów dla każdej z warstw, która obejmuje wszystkie możliwe wagi.
Dalsze ustalanie precyzji można wykonać w trakcie analizy wyników
symulacji.

\subsection{GStreamer}
Zsyntetyzowana sieć jest częścią projektu. Potrzebne również dostarczenie
danych do sieci oraz przedstawienie wyniku.
Do tego celu skorzystano z biblioteki GStreamer, dzięki której można
\todo{Graf --- schemat nr 2}
tworzyć grafy z komponentów (pluginów, elementów)
przetwarzających media, zarówno audio jak i video.
Każdy z elementów grafu składa się z co najmniej jednego źródła (source),
lub ujścia (sink), może mieć również wiele wejść i wyjść. W grafie pierwszy
element nie może mieć wejść, natomiast konieczne jest aby posiadał co najmniej
jedno wyjście. Poprawnie przygotowany graf nie powinien mieć komponentów
oferujących źródło, które nie są z niczym połączone.
Pluginy mają ujednolicony interfejs, dzięki czemu można w łatwy sposób
włączyć do grafu własny elemetn. Wtyczki charakteryzują się
pewnymi własnościami, znanymi jako „caps”. Określają one jakie parametry
muszą spełniać media, aby zostały przetworzone przez daną wtyczkę.
Łączone ze sobą elementy dokonują negocjacji
parametrów mediów, takich jak rozdzielczość obrazu, format pikseli,
ilość klatek na sekundę oraz innych.

Wszystkie z elementów użytego grafu opiszę pokrótce:
\todo[inline]{Alfabetycznie czy zgodnie z grafem?}

\subsubsection{Caps filter}
Element precyzujący parametry obrazu, które wymuszają
dostosowanie się poprzedniego elementu --- na przykład videoscale.
Zapisuje się je w postaci ciągu znaków objętych w cudzysłów.

\subsubsection{fpsdisplaysink}
Wtyczka typu sink (mająca tylko ujście), która jako parametr
pobiera inną wtyczkę tego typu, np. xlnxvideosink. Jej użycie
pozwala na sprawdzenie liczby klatek na sekundę wyświetlanego
obrazu.

\subsubsection{videoconvert}
Element mający za zadanie dostosować wszystkie parametry obrazu tak,
aby móc połączyć ze sobą dwa niekompatybilne pod względem „caps” elementy.
Ta niekompatybilność może być spowodowana na przykład tym, że dwie
wtyczki potrzebują innego formatu pikseli i jednocześnie nie oferują
możliwości konwersji z jednego formatu na inny.

\subsubsection{videocrop}
Wtyczka służąca do wykadrowania obrazu w zdefiniowanym obszarze.
Wykorzystana została aby otrzymać obraz o tej samej długości
i szerokości wynoszącej 224 (co jest ośmiokrotnością 28, czyli
długością boku obrazów, którymi wytrenowana została sieć) wycięty
ze środka wideo o rozmiarze 1920\(\times \)1080.

\subsubsection{videoscale}
Skaluje obraz do wynegocjowanych pomiędzy sąsiadującymi elementami
parametrów, przy czym pierwsza próba negocjacji to ta sama wielkość
obrazu przy ujściu jak i w źródle, aby skalowanie nie było potrzebne.

\subsubsection{videobox}
Oprócz możliwości videocrop oferuje także możliwość osadzenia
przychodzącego wideo w tym o większym rozmiarze dodając dookoła ramkę
w wybranym kolorze. Własność autocrop oznacza automatyczne obliczenie
wielkości ramek na podstawie parametrów określonych przez kolejny element tak,
aby obraz przychodzący do videobox był wycentrowany a ramki
były tej samej wielkości.

\subsubsection{xlnxvideosrc i xlnxvideosink}
Są to pluginy dostarczone przez firmę Xilinx wraz z platformą reVISION.
Obydwa korzystają biblioteki Xilinx \lstinline{video_lib}
Pierwszy z nich ułatwia odczytywanie danych ze źródeł, dla których potrzebne
byłyby dodatkowe działania. Są to między innymi kamera USB (użyta w projekcie),
HDMI, MIPI CSI (sprzętowy interfejs do transmisji obrazów i wideo).
\todo{Przypis UG1221, s.32}
Sam element zbudowany jest w oparciu o element v4l2src, dostępny
w standardowej instalacji GStreamera.
Xlnxvideosink również jest oparty o inny element --- kmssink.
Zapewnia odpowiednią konfigurację połączenia z wyświetlaczami
podłączonymi przez HDMI oraz DisplayPort.


\subsection{Używanie sieci}
Czyli synteza sieci i zrobienie z niej biblioteki statycznej „.a” 
\subsubsection{Dostosowanie sieci}
W celu poprawienia wyników działania sieci dokonano pewnych usprawnień.
Rzeczywiste dane z kamery mogą być zaszumione, obiekty zacienione.
A biały nie jest białym.


\subsection{Część neuralnet}
Czytanie obrazu, podział na część luma i chroma, wywołanie funkcji sieci,
zapis z powrotem, synteza do biblioteki dzielonej „.so”

\subsection{Część gstsdxnet}
De facto plugin gstreamera, w którym są wywoływane funkcje z biblioteki
dzielonej neuralnet.so, 

\subsection{Małe podsumowanie}


\newpage
\section{Wyniki i dyskusja}

\subsection{Ewaulacja modelu}
Wyniki z samego pythona z danymi testowymi z mnista

\subsection{Symulacja}
Tutaj wyniki z symulacji z danymi testowymi z mnista

\subsection{Dane rzeczywiste}
Wyniki z kamerki. Zdjęcia danych testowych, co wpływa na wynik, czy wszystko
rozpoznaje itd,

\newpage
\section{Podsumowanie}
W projekcie zostało zrobione to i to. Wyszło to tak i tak. Problem sprawiło
tamto i owamto. Można to poprawić w ten sposób. Można część funkcjonalności
z pipeline przenieść na fpga (w końcu przetwarzanie obrazu na fpga jest szybkie)

\end{document}